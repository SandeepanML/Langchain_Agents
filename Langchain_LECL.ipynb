{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82e515ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4a660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'llama3.2:1b'\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=base_url,\n",
    "    model = model,\n",
    "    temperature=0.8,\n",
    "    num_predict = 256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccc34f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 + 5 = 7'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('What is 2+5').content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54fc1f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = \"\"\n",
    "\n",
    "for chunks in llm.stream(\"how to Fine tune a model, explain in 3 steps\"):\n",
    "    response = response + \" \" + chunks.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "801b2e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Fine -t uning  a  model  is  the  process  of  adjusting  the  parameters  and  training  data  to  improve  its  performance  on  a  specific  task  or  problem .  Here  are  three  steps  to  fine -t une  a  model :\\n\\n ** Step   1 :  Data  Aug mentation **\\n\\n Data  augmentation  is  the  process  of  artificially  increasing  the  size  and  diversity  of  your  dataset  by  applying  transformations  such  as  rotation ,  scaling ,  flipping ,  color  jitter ing ,  etc .  This  helps  the  model  learn  more  general izable  patterns  in  the  data  and  reduces  over fit ting .\\n\\n *  Apply  these  augment ations  to  your  training  dataset  several  times \\n *  Keep  track  of  metrics  such  as  accuracy ,  precision ,  recall ,  F 1  score ,  etc .\\n *  Use  tools  like  TensorFlow  or  Py T orch 's  ` ImageData Generator `  to  automate  this  process \\n\\n ** Step   2 :  Hyper parameter  Tun ing **\\n\\n Hyper parameters  are  the  model 's  parameters  that  control  its  behavior .  Fine -t uning  involves  adjusting  these  hyper parameters  to  find  the  optimal  settings  for  your  specific  problem .\\n\\n *  Try  different  values  of  hyper parameters  such  as  learning  rate ,  batch  size ,  number  of  epochs ,  etc .\\n *  Use  techniques  like  grid  search  or  random  search  to  explore  a  wide  range  of  options \\n *  Monitor  performance  metrics  such  as  loss ,  accuracy ,  and  F 1  score \\n\\n ** Step   3 :  Model  Architecture  and  Training **\\n\\n Once  you  have  adjusted  the  hyper parameters \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78dba7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three key points explaining a chemical reaction:\n",
      "\n",
      "1. **Chemical Reaction Definition**: A chemical reaction is a process in which one or more substances (reactants) are converted into new substances (products). This conversion involves the breaking and forming of chemical bonds between atoms, resulting in a change of state from reactant to product.\n",
      "\n",
      "2. **Types of Chemical Reactions**: There are several types of chemical reactions, including:\n",
      "* Synthesis reactions: where two or more substances combine to form a new compound.\n",
      "* Decomposition reactions: where a single substance breaks down into two or more simpler substances.\n",
      "* Combustion reactions: where a substance reacts with oxygen to produce heat and light.\n",
      "\n",
      "3. **Factors That Influence Chemical Reactions**: Several factors can influence the rate, equilibrium, and conditions of chemical reactions, including:\n",
      "* Concentration of reactants and products\n",
      "* Temperature\n",
      "* Pressure\n",
      "* Catalysts or inhibitors\n",
      "* Surface area and contact time\n",
      "\n",
      "These are just a few key points that explain chemical reactions. If you'd like me to expand on any of these topics or provide further examples, please let me know!\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'llama3.2:1b'\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=base_url,\n",
    "    model = model,\n",
    "    temperature=0.8,\n",
    "    num_predict = 256\n",
    ")\n",
    "\n",
    "question = \"Explain a Chemical reaction in 3 points\"\n",
    "response = llm.invoke(question)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e2695c",
   "metadata": {},
   "source": [
    "## LangChain Chat Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7c90532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'llama3.2:1b'\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=base_url,\n",
    "    model = model,\n",
    "    temperature=0.8,\n",
    "    num_predict = 256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6657841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = SystemMessage('You are an Elementary School Teacher, explain the kids about values in a very ethical manner')\n",
    "userquestion = HumanMessage('If you find a kid pushing other kid (classmate), how will you respond ?')\n",
    "\n",
    "messages = [system, userquestion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0edf3ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73fcccd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an elementary school teacher, my goal is to create a safe and respectful learning environment for all students. If I were to witness a student finding a classmate being pushed, my response would be guided by the principles of empathy, kindness, and fairness.\n",
      "\n",
      "Firstly, I would immediately speak with the student who was pushing the other child and ask them why they thought it was okay to do so. It's essential to understand their perspective and help them see how their actions affected others.\n",
      "\n",
      "Here's an example of how I might respond:\n",
      "\n",
      "\"Hey [student], I noticed what you did earlier today, and I want to talk to you about it. Pushing someone can hurt their feelings and make them feel uncomfortable. Can you tell me why you thought it was okay to push your friend that way?\"\n",
      "\n",
      "By asking this question, I'm encouraging the student to reflect on their behavior and consider how their actions might impact others.\n",
      "\n",
      "Next, I would listen attentively to the student's response and address any misconceptions they may have had about pushing. It's essential to teach children that everyone has different boundaries and feelings, and it's never okay to push someone who doesn't want to be pushed.\n",
      "\n",
      "If the student is willing to apologize and make amends, I might allow them to apologize\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07df610",
   "metadata": {},
   "source": [
    "## *** VVI - Use Prompt Template to Make it more Flexible (By Adding Variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc731ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (SystemMessagePromptTemplate,\n",
    "                               HumanMessagePromptTemplate,\n",
    "                               PromptTemplate,\n",
    "                               ChatPromptTemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16e1531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = SystemMessagePromptTemplate.from_template(\"You are a {type} teacher. You answer in short sentences.\")\n",
    "human = HumanMessagePromptTemplate.from_template(\"Tell me about the {topics} in {numbers} points.\")\n",
    "\n",
    "system.format(type = \"10th Grade Teacher\")\n",
    "human.format(topics = \"Sun\", numbers = 2)\n",
    "\n",
    "messages = [system, human]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43bcd535",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76dcf520",
   "metadata": {},
   "outputs": [],
   "source": [
    "completeChat = template.invoke({'type':'elementary', 'topics': 'Sun', 'numbers':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adc12f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(completeChat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8065b2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's what I know about the Sun:\n",
      "\n",
      "1. The Sun is the star at the center of our solar system.\n",
      "2. It makes up 99.8% of the mass in our solar system.\n",
      "3. The Sun is a huge ball of hot, glowing gas.\n",
      "4. It shines because it converts sunlight into heat and light.\n",
      "5. Without the Sun, life on Earth would be very different!\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2342a8c1",
   "metadata": {},
   "source": [
    "## -------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78ca7b",
   "metadata": {},
   "source": [
    "## *** VVI - Langchain Expression Language (LECL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2b8ff",
   "metadata": {},
   "source": [
    "#### (1) Sequential LECL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0efc52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import (SystemMessagePromptTemplate,\n",
    "                               HumanMessagePromptTemplate,\n",
    "                               PromptTemplate,\n",
    "                               ChatPromptTemplate)\n",
    "\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'llama3.2:1b'\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=base_url,\n",
    "    model = model,\n",
    "    temperature=0.8,\n",
    "    num_predict = 256\n",
    ")\n",
    "\n",
    "system = SystemMessagePromptTemplate.from_template(\"You are a {type} teacher. You answer in short sentences.\")\n",
    "human = HumanMessagePromptTemplate.from_template(\"Tell me about the {topics} in {numbers} points.\")\n",
    "\n",
    "system.format(type = \"10th Grade Teacher\")\n",
    "human.format(topics = \"Sun\", numbers = 2)\n",
    "\n",
    "messages = [system, human]\n",
    "\n",
    "template = ChatPromptTemplate(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "007c0f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template | llm\n",
    "response = chain.invoke({'type':'elementary', 'topics': 'Sun', 'numbers':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72613def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are five key things to know about the Sun:\n",
      "\n",
      "*   It's really, really big - it's bigger than our whole Earth!\n",
      "*   It's the center of our solar system and makes everything warm.\n",
      "*   The Sun gives us light, heat, and energy every day.\n",
      "*   We need to protect ourselves from its strong rays by wearing sunscreen and sunglasses when we go outside during peak hours.\n",
      "*   A new Sun will eventually be born in about 5 billion years, but for now it's just happy to shine.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fcda070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2703ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template | llm | StrOutputParser()\n",
    "response = chain.invoke({'type':'elementary', 'topics': 'Sun', 'numbers':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f488bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's what I know about the Sun:\n",
      "\n",
      "1. The Sun is the star at the center of our solar system.\n",
      "2. It makes up about 99.8% of the mass in our planet Earth.\n",
      "3. The Sun is huge, with a diameter of over 1,392,684 kilometers (865,374 miles).\n",
      "4. It takes the Sun about 8 minutes and 20 seconds to complete one rotation on its axis.\n",
      "5. The Sun's surface temperature is about 5,500 degrees Celsius (10,000 degrees Fahrenheit).\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc91c33",
   "metadata": {},
   "source": [
    "## ***VVI\n",
    "\n",
    "## Chaining Runnables (Chain Multipe Runnables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06891f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import (SystemMessagePromptTemplate,\n",
    "                               HumanMessagePromptTemplate,\n",
    "                               PromptTemplate,\n",
    "                               ChatPromptTemplate)\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'llama3.2:1b'\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=base_url,\n",
    "    model = model,\n",
    "    temperature=0.8,\n",
    "    num_predict = 256\n",
    ")\n",
    "\n",
    "system = SystemMessagePromptTemplate.from_template(\"You are a {type} teacher. You answer in short sentences.\")\n",
    "human = HumanMessagePromptTemplate.from_template(\"Tell me about the {topics} in {numbers} points.\")\n",
    "\n",
    "system.format(type = \"10th Grade Teacher\")\n",
    "human.format(topics = \"Sun\", numbers = 2)\n",
    "\n",
    "messages = [system, human]\n",
    "\n",
    "template = ChatPromptTemplate(messages)\n",
    "chain1 = template | llm | StrOutputParser()\n",
    "\n",
    "response = chain1.invoke({'type':'elementary', 'topics': 'Sun', 'numbers':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ec4680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysys_prompt = ChatPromptTemplate.from_template('''analyse the following text: {response}.\n",
    "                                                   You will need to tell me on a scale of 1 to 10 how easy it is to comprehend.\n",
    "                                                   Answer in one line please.''')\n",
    "composed_chain = {\"response\":chain1} | analysys_prompt | llm | StrOutputParser()\n",
    "output = composed_chain.invoke({'type':'phd', 'topics': 'Sun', 'numbers':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "360345e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd rate the comprehensibility of this text as an 8, requiring some effort and knowledge of astronomy to fully grasp the points about the Sun.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09e1cfe",
   "metadata": {},
   "source": [
    "## ***VVI\n",
    "## Parallel LECL LangChain\n",
    "- Parallel chains are used to run multiple runnables in parallel\n",
    "- The final return value is a dict with the results of each value under its appropriate key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34ea900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = SystemMessagePromptTemplate.from_template('You are a {school} teacher, you answer in short sentences')\n",
    "human1 = HumanMessagePromptTemplate.from_template('Tell me about the {topic} in {lines} points.')\n",
    "\n",
    "message1 = [system, human1]\n",
    "template1 = ChatPromptTemplate(message1)\n",
    "fact_chain = template1 | llm | StrOutputParser()\n",
    "\n",
    "output = fact_chain.invoke({'school':'primary', 'topic':'solar system', 'lines':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ff62a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun at center shines bright,\n",
      "Planets orbit around it with delight.\n"
     ]
    }
   ],
   "source": [
    "human2 = HumanMessagePromptTemplate.from_template('Construct a poem on {topic} in {sentences} lines.')\n",
    "\n",
    "messages2 = [system, human2]\n",
    "template2 = ChatPromptTemplate(messages2)\n",
    "poem_chain = template2 | llm | StrOutputParser()\n",
    "\n",
    "output = poem_chain.invoke({'school':'primary', 'topic':'solar system', 'sentences':2})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "65236e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0116e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "primechain = RunnableParallel(fact = fact_chain, poem = poem_chain)\n",
    "output = primechain.invoke({'school':'primary', 'topic':'solar system', 'lines':2, 'sentences':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "38b1edb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun is the center cold\n",
      "Planets orbiting around it slowly.\n"
     ]
    }
   ],
   "source": [
    "print(output[\"poem\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e0833b",
   "metadata": {},
   "source": [
    "## *** VVI\n",
    "## Chain Router\n",
    "- The router chain is used to route the output of a previous runnable to the next runnable based on the output of the Previous runnable\n",
    "- **Runnable Lambda** : Passing your input from one chain to the another chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2349a6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
    "\n",
    "prompt = \"\"\"Given the user review below, classify it as either being about 'Positive' and 'Negative'.\n",
    "            Do not respond with more than one word,\n",
    "\n",
    "            Review: {review}\n",
    "            Classification:\"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "chain = template | llm | StrOutputParser()\n",
    "\n",
    "review = \"Thank you so much for providing such a great platform for learning. I am happy with the Service.\"\n",
    "\n",
    "chain.invoke({'review' : review})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "72508177",
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveprompt = \"\"\"\n",
    "                you are an expert in writing reply for positive reviews. You need to encourage the user to share their \n",
    "                experience on social media.\n",
    "                Review: {review}\n",
    "                Answer:\"\"\"\n",
    "\n",
    "positive_template = ChatPromptTemplate.from_template(positiveprompt)\n",
    "positivecall = positive_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e0eac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "negativeprompt = \"\"\"\n",
    "                you are an expert in writing reply for negative reviews. You need to first apologize for the inconvinience caused to the user.\n",
    "                Then encourage the user to share their concern on the following email: 'checkout@abc.com'.\n",
    "                Review: {review}\n",
    "                Answer:\"\"\"\n",
    "\n",
    "negative_template = ChatPromptTemplate.from_template(negativeprompt)\n",
    "negativecall = negative_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "59530be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route(info):\n",
    "    if \"positive\" in info[\"sentiment\"].lower():\n",
    "        return positivecall\n",
    "    else:\n",
    "        return negativecall\n",
    "\n",
    "# route({'sentiment':'positive'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a24d626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_chain = {'sentiment': chain, 'review':lambda x: x['review']} | RunnableLambda(route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "308be229",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_review = \"The service was prompt and friendly. Definitely a good experience!\"\n",
    "neg_review = \"While the product works well overall, I was disappointed with the delayed delivery and some packaging issues.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9ec6ad8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a draft reply:\n",
      "\n",
      "Subject: Thank you for taking the time to share your feedback about our checkout experience.\n",
      "\n",
      "Dear [User],\n",
      "\n",
      "I am writing to apologize sincerely for the inconvenience you've experienced with our recent order. We are truly sorry that the delayed delivery and some packaging issues have affected your overall satisfaction with our service. At [Your Company Name], we strive to provide the best possible experience for all our customers, and it's clear that we fell short of this standard in your case.\n",
      "\n",
      "I would like to extend an invitation for you to share your full satisfaction with us through a separate email to help us improve and prevent similar issues in the future. Your feedback is invaluable to us, and I am confident that by working together, we can resolve these concerns and continue to deliver excellent service.\n",
      "\n",
      "Please let me know if there's anything else we can do to make things right. You can reach out directly to us at checkout@abc.com with any additional information or questions you may have. We appreciate your patience and understanding in this matter, and I look forward to the opportunity to turn things around for you.\n",
      "\n",
      "Thank you again for taking the time to share your feedback.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "Customer Service Team\n",
      "[Your Company Name]\n"
     ]
    }
   ],
   "source": [
    "output = complete_chain.invoke({'review': neg_review})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e63a98a",
   "metadata": {},
   "source": [
    "## *** VVI\n",
    "## Make a Custom ChainRunnables with RunnablePassThrough and RunnableLambda\n",
    "\n",
    "- This is used when you need functionality not provided by other LangChain components, and custom functions used as Runnables and called as RunnableLambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dead3418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel, RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0d80ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_counts(text):\n",
    "    return len(text)\n",
    "\n",
    "def word_counts(text):\n",
    "    return len(text.split())\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Explain these inputs in 2 sentences: {input1} and {input2}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "89309e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "newchain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1d94964c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The statement \"Earth is a planet\" refers to the fact that our home planet, specifically, is an independent celestial body orbiting a main-sequence star, known as the Sun. The phrase \"Sun is a star\" describes the same star at which Earth orbits, with all three statements conveying information about the celestial bodies in our solar system and their characteristics.\n"
     ]
    }
   ],
   "source": [
    "output = newchain.invoke({'input1': 'Earth is a Planet', 'input2': 'Sun is a star'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "358857d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "improvedchain = prompt | llm | StrOutputParser() | {'char_counts': RunnableLambda(char_counts),\n",
    "                                                    'word_counts': RunnableLambda(word_counts),\n",
    "                                                    'main_output': RunnablePassthrough()\n",
    "                                                    }\n",
    "\n",
    "output = improvedchain.invoke({'input1': 'Earth is a Planet', 'input2': 'Sun is a star'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "73d230ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'char_counts': 455, 'word_counts': 81, 'main_output': 'The statement \"Earth is a Planet\" can be broken down as follows: It\\'s suggesting that the planet we know and call Earth, which orbits around our sun (a star) and maintains its shape due to gravity, is indeed classified as a planet in our solar system. This classification primarily refers to objects that are large enough to be rounded by their own gravity but have not cleared their orbital zone of other objects, like the gas giants in our solar system.'}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f260de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d5e116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a6519c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed4e999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072ab97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53eb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6be73e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e95760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bea0438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d81fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c33319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
